{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c755bf4",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> ON THE MARKET </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b5244",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> Sales </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d870e",
   "metadata": {},
   "source": [
    "First things first, we will import the required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e418af4",
   "metadata": {},
   "source": [
    "Note: You have scrapped for Agents yet\n",
    "        The next button is not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0610b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8278900",
   "metadata": {},
   "source": [
    "Next, we will load in the dataset containing the list of postcode for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91de3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_csv('London postcode districts.xlsx - PC DIST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb8bdb",
   "metadata": {},
   "source": [
    "Now, we will create a function called 'OnTheMarket_sales' to scrap properties that are for sale on the OnTheMarket website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a39821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Find the search bar and enter the search query\n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(13)\n",
    "    #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    \n",
    "    # Scrapping data for the required features in the first page\n",
    "    time.sleep(2)\n",
    "    print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "    address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "    bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "    bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "    price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "    desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "    agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "    property_url_list = driver.current_url\n",
    "    Trans_type_list = Trans_type\n",
    "    website_list = website\n",
    "    for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "        address.append(address_item.text)\n",
    "        types.append(type_item.text)\n",
    "        bedrooms.append(bedroom_item.text)\n",
    "        bathrooms.append(bathroom_item.text)\n",
    "        prices.append(price_item.text)\n",
    "        desc.append(address_item.text + '. ' + type_item.text)\n",
    "        date_added.append(date_added_item.text)\n",
    "        agent_list.append(agent_list_item.text)\n",
    "        property_url.append(property_url_list)\n",
    "        Trans_type.append(Trans_type_list)\n",
    "        website.append(website_list)\n",
    "\n",
    "      \n",
    "    time.sleep(1.3)\n",
    "    # get the height of the page\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "    # scroll to the middle of the page using JavaScript\n",
    "    driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "    \n",
    "    # Click to cancel the pop-up window \n",
    "    time.sleep(1.5)\n",
    "    driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "    time.sleep(1.1) \n",
    "    next_botton = driver.find_element(By.XPATH, '//div/a/button') \n",
    "    \n",
    "    \n",
    "    last_botton_check = driver.find_element(By.XPATH, '//div/button') \n",
    "    html = last_botton_check.get_attribute('outerHTML') \n",
    "    if 'disabled' in html:\n",
    "        print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "    \n",
    "    time.sleep(1.3)\n",
    "    next_botton.click()\n",
    "    \n",
    "    # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=2\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        # check if current page is the last page. If true break away from the loop\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton.click()\n",
    "            \n",
    "            #Increment the page number\n",
    "            i += 1\n",
    "            \n",
    "        except:\n",
    "            print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "            break\n",
    "            \n",
    "        time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_sales(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2233b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1 from WC2B\n",
      "scraping page 2 from WC2B\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2B\n",
      "Total numbers of properties available in WC2B is 6\n",
      "scraping page 1 from WC2E\n",
      "scraping page 2 from WC2E\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2E\n",
      "Total numbers of properties available in WC2E is 6\n",
      "scraping page 1 from WC2H\n",
      "scraping page 2 from WC2H\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2H\n",
      "Total numbers of properties available in WC2H is 6\n",
      "scraping page 1 from WC2N\n",
      "scraping page 2 from WC2N\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2N\n",
      "Total numbers of properties available in WC2N is 8\n",
      "scraping page 1 from WC2R\n",
      "scraping page 2 from WC2R\n",
      "scraping page 3 from WC2R\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2R\n",
      "Total numbers of properties available in WC2R is 12\n",
      "------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS 38\n"
     ]
    }
   ],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes.loc[263:]['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_sales(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a11b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hi4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fc34a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postcode district</th>\n",
       "      <th>Local Areas</th>\n",
       "      <th>Borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>WC2B</td>\n",
       "      <td>Drury Lane, Aldwych</td>\n",
       "      <td>Camden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>WC2E</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>WC2H</td>\n",
       "      <td>Leicester Square</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>WC2N</td>\n",
       "      <td>Charing Cross</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>WC2R</td>\n",
       "      <td>Somerset House</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Postcode district          Local Areas      Borough\n",
       "263              WC2B  Drury Lane, Aldwych       Camden\n",
       "264              WC2E        Covent Garden  Westminster\n",
       "265              WC2H     Leicester Square  Westminster\n",
       "266              WC2N        Charing Cross  Westminster\n",
       "267              WC2R       Somerset House  Westminster"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = codes.loc[263:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94c815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Find the search bar and enter the search query\n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(10)\n",
    "    #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "   \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    \n",
    "    # Scrapping data for the required features in the first page\n",
    "    time.sleep(2)\n",
    "    print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "    address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "    bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "    bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "    price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "    desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "    agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "    property_url_list = driver.current_url\n",
    "    Trans_type_list = Trans_type\n",
    "    website_list = website\n",
    "    for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "        address.append(address_item.text)\n",
    "        types.append(type_item.text)\n",
    "        bedrooms.append(bedroom_item.text)\n",
    "        bathrooms.append(bathroom_item.text)\n",
    "        prices.append(price_item.text)\n",
    "        desc.append(address_item.text + '. ' + type_item.text)\n",
    "        date_added.append(date_added_item.text)\n",
    "        agent_list.append(agent_list_item.text)\n",
    "        property_url.append(property_url_list)\n",
    "        Trans_type.append(Trans_type_list)\n",
    "        website.append(website_list)\n",
    "\n",
    "      \n",
    "    time.sleep(1.3)\n",
    "    # get the height of the page\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "    # scroll to the middle of the page using JavaScript\n",
    "    driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "    \n",
    "    # Click to cancel the pop-up window \n",
    "    time.sleep(1.5)\n",
    "    driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "    #time.sleep(1.1) \n",
    "    #next_botton = driver.find_element(By.XPATH, '//div/a/button') \n",
    "    \n",
    "    \n",
    "    #last_botton_check = driver.find_element(By.XPATH, '//div/button') \n",
    "    #html = last_botton_check.get_attribute('outerHTML') \n",
    "    #if 'disabled' in html:\n",
    "    print('Done')\n",
    "    \n",
    "    #time.sleep(1.3)\n",
    "    #next_botton.click()\n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=2\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        # check if current page is the last page. If true break away from the loop\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton.click()\n",
    "            \n",
    "            #Increment the page number\n",
    "            i += 1\n",
    "            \n",
    "        except:\n",
    "            print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "            break\n",
    "            \n",
    "        time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9f5bc",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> Rent </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "19cfc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #click on Rent option \n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[1]/button[2]').click()\n",
    "    time.sleep(1.2)\n",
    "\n",
    "    # Find the search bar  \n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    time.sleep(1.3)\n",
    "    \n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(8) \n",
    "    driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    \n",
    "    # Scrapping data for the required features in the first page\n",
    "    time.sleep(2)\n",
    "    print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "    address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a') \n",
    "    type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\") \n",
    "    bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]') \n",
    "    bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]') \n",
    "    price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a') \n",
    "    desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "    agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "    property_url_list = driver.current_url\n",
    "    Trans_type_list = Trans_type\n",
    "    website_list = website\n",
    "    for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "        address.append(address_item.text)\n",
    "        types.append(type_item.text)\n",
    "        bedrooms.append(bedroom_item.text)\n",
    "        bathrooms.append(bathroom_item.text)\n",
    "        prices.append(price_item.text)\n",
    "        desc.append(address_item.text + '. ' + type_item.text)\n",
    "        date_added.append(date_added_item.text)\n",
    "        agent_list.append(agent_list_item.text)\n",
    "        property_url.append(property_url_list)\n",
    "        Trans_type.append(Trans_type_list)\n",
    "        website.append(website_list)\n",
    "\n",
    "      \n",
    "    time.sleep(1.3)\n",
    "    # get the height of the page\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "    # scroll to the middle of the page using JavaScript\n",
    "    driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "    \n",
    "    # Click to cancel the pop-up window \n",
    "    time.sleep(1.5)\n",
    "    driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "    time.sleep(1.1) \n",
    "    next_botton = driver.find_element(By.XPATH, '//div/a/button') \n",
    "    \n",
    "    \n",
    "    last_botton_check = driver.find_element(By.XPATH, '//div/button') \n",
    "    html = last_botton_check.get_attribute('outerHTML') \n",
    "    if 'disabled' in html:\n",
    "        print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "    \n",
    "    time.sleep(1.3)\n",
    "    next_botton.click()\n",
    "    \n",
    "    # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=2\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        # check if current page is the last page. If true break away from the loop\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton.click()\n",
    "            \n",
    "            #Increment the page number\n",
    "            i += 1\n",
    "            \n",
    "        except:\n",
    "            print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "            break\n",
    "            \n",
    "        time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Rent','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "96d0ab90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1 from CR3\n",
      "------------------------------- SCRAPING COMPLETED FOR CR3\n",
      "Total numbers of properties available in CR3 is 10\n",
      "scraping page 1 from CR4\n",
      "scraping page 2 from CR4\n",
      "------------------------------- SCRAPING COMPLETED FOR CR4\n",
      "Total numbers of properties available in CR4 is 37\n",
      "scraping page 1 from CR5\n",
      "------------------------------- SCRAPING COMPLETED FOR CR5\n",
      "Total numbers of properties available in CR5 is 17\n",
      "scraping page 1 from CR6\n",
      "------------------------------- SCRAPING COMPLETED FOR CR6\n",
      "Total numbers of properties available in CR6 is 17\n",
      "scraping page 1 from CR7\n",
      "scraping page 2 from CR7\n",
      "scraping page 3 from CR7\n",
      "------------------------------- SCRAPING COMPLETED FOR CR7\n",
      "Total numbers of properties available in CR7 is 49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes.loc[8:]['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_sales(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2b9af3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hey2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2400f58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postcode district</th>\n",
       "      <th>Local Areas</th>\n",
       "      <th>Borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>CR0</td>\n",
       "      <td>Croydon, Addiscombe, Shirley, Addington, New A...</td>\n",
       "      <td>Croydon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>CR2</td>\n",
       "      <td>South Croydon, Sanderstead, Selsdon, Addington</td>\n",
       "      <td>Croydon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>CR3</td>\n",
       "      <td>Caterham, Whyteleafe, Chaldon, Woldingham</td>\n",
       "      <td>Croydon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>CR4</td>\n",
       "      <td>Mitcham, Beddington Corner</td>\n",
       "      <td>Merton</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>CR5</td>\n",
       "      <td>Coulsdon, Chipstead, Woodmansterne</td>\n",
       "      <td>Croydon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>WC2B</td>\n",
       "      <td>Drury Lane, Aldwych</td>\n",
       "      <td>Camden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>WC2E</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>WC2H</td>\n",
       "      <td>Leicester Square</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>WC2N</td>\n",
       "      <td>Charing Cross</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>WC2R</td>\n",
       "      <td>Somerset House</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>260 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Postcode district                                        Local Areas  \\\n",
       "8                 CR0  Croydon, Addiscombe, Shirley, Addington, New A...   \n",
       "9                 CR2     South Croydon, Sanderstead, Selsdon, Addington   \n",
       "10                CR3          Caterham, Whyteleafe, Chaldon, Woldingham   \n",
       "11                CR4                         Mitcham, Beddington Corner   \n",
       "12                CR5                 Coulsdon, Chipstead, Woodmansterne   \n",
       "..                ...                                                ...   \n",
       "263              WC2B                                Drury Lane, Aldwych   \n",
       "264              WC2E                                      Covent Garden   \n",
       "265              WC2H                                   Leicester Square   \n",
       "266              WC2N                                      Charing Cross   \n",
       "267              WC2R                                     Somerset House   \n",
       "\n",
       "         Borough  \n",
       "8        Croydon  \n",
       "9        Croydon  \n",
       "10       Croydon  \n",
       "11        Merton  \n",
       "12       Croydon  \n",
       "..           ...  \n",
       "263       Camden  \n",
       "264  Westminster  \n",
       "265  Westminster  \n",
       "266  Westminster  \n",
       "267  Westminster  \n",
       "\n",
       "[260 rows x 3 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = codes.loc[8:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "639a6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #click on Rent option \n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[1]/button[2]').click()\n",
    "    time.sleep(1.2)\n",
    "\n",
    "    # Find the search bar  \n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    time.sleep(1.3)\n",
    "    \n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(10)\n",
    "    #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "   \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    \n",
    "    # Scrapping data for the required features in the first page\n",
    "    time.sleep(2)\n",
    "    print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "    address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "    bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "    bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "    price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a')\n",
    "    desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "    agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "    property_url_list = driver.current_url\n",
    "    Trans_type_list = Trans_type\n",
    "    website_list = website\n",
    "    for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "        address.append(address_item.text)\n",
    "        types.append(type_item.text)\n",
    "        bedrooms.append(bedroom_item.text)\n",
    "        bathrooms.append(bathroom_item.text)\n",
    "        prices.append(price_item.text)\n",
    "        desc.append(address_item.text + '. ' + type_item.text)\n",
    "        date_added.append(date_added_item.text)\n",
    "        agent_list.append(agent_list_item.text)\n",
    "        property_url.append(property_url_list)\n",
    "        Trans_type.append(Trans_type_list)\n",
    "        website.append(website_list)\n",
    "\n",
    "      \n",
    "    time.sleep(1.3)\n",
    "    # get the height of the page\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "    # scroll to the middle of the page using JavaScript\n",
    "    driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "    \n",
    "    # Click to cancel the pop-up window \n",
    "    time.sleep(1.5)\n",
    "    driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "    #time.sleep(1.1) \n",
    "    #next_botton = driver.find_element(By.XPATH, '//div/a/button') \n",
    "    \n",
    "    \n",
    "    #last_botton_check = driver.find_element(By.XPATH, '//div/button') \n",
    "    #html = last_botton_check.get_attribute('outerHTML') \n",
    "    #if 'disabled' in html:\n",
    "    print('Done')\n",
    "    \n",
    "    #time.sleep(1.3)\n",
    "    #next_botton.click()\n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Rent','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=2\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        # check if current page is the last page. If true break away from the loop\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton.click()\n",
    "            \n",
    "            #Increment the page number\n",
    "            i += 1\n",
    "            \n",
    "        except:\n",
    "            print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "            break\n",
    "            \n",
    "        time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e78c987e",
   "metadata": {},
   "source": [
    "This is the master script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "0076e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #click on Rent option \n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[1]/button[2]').click()\n",
    "    time.sleep(1.2)\n",
    "\n",
    "    # Find the search bar  \n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    time.sleep(1.3)\n",
    "    \n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(13) \n",
    "    try:\n",
    "        driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    except:\n",
    "        driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[2]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    while True:\n",
    "        # Scrapping data for the required features in the first page\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "\n",
    "        time.sleep(1.3)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//1.37});\")\n",
    "\n",
    "        # Click to cancel the pop-up window \n",
    "        time.sleep(1.5)\n",
    "        try:\n",
    "            time.sleep(1.6)\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button')\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "            time.sleep(1.1)\n",
    "        except:\n",
    "            time.sleep(1.5)\n",
    "\n",
    "\n",
    "        # Find the next button\n",
    "        if i==1:\n",
    "            try:\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "\n",
    "                # Find the last next button \n",
    "                last_botton_check = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "        else:\n",
    "            try:\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "\n",
    "                # Find the last next button \n",
    "                last_botton_check = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "            \n",
    "        #Click the next button if it's not the last \n",
    "        time.sleep(5.5)\n",
    "        #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "        next_botton.click()\n",
    "        \n",
    "        #Increment the page number\n",
    "        i += 1\n",
    "        time.sleep(2)\n",
    "                \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Rent','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ac42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "<button class=\"otm-Button whitespace-nowrap py-2 leading-normal h-auto\n",
    "shadow-none font-heading font-semibold text-center justify-center inli\n",
    "ne-flex items-center px-8 rounded-md border-2 disabled:bg-dove disabled\n",
    "    :text-white disabled:border-transparent hover:disabled:opacity-100 \n",
    "                    transition duration-200 ease-in-out border-burnt-\n",
    "                    coral text-burnt-coral hover:opacity-70  min-h-[\n",
    "52px] text-md md:min-h-[70px] sm:text-base2\">Next</button>\n",
    "                                \n",
    "                                \n",
    "<button class=\"otm-Button whitespace-nowrap py-2 leading-normal h-auto\n",
    "shadow-none font-heading font-semibold text-center justify-center inli\n",
    "ne-flex items-center px-8 rounded-md border-2 disabled:bg-dove disabled\n",
    "    :text-white disabled:border-transparent hover:disabled:opacity-100\n",
    "                    transition duration-200 ease-in-out border-burnt-\n",
    "                    coral text-burnt-coral hover:opacity-70 order-last min-h-[52px]\n",
    "text-md md:min-h-[70px] sm:text-base2\" disabled=\"\">Next</button>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Find the search bar and enter the search query\n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "   \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "     # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=1\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + ' ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        if i == 1:\n",
    "             # Click to cancel the pop-up window \n",
    "            time.sleep(1.5)\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "            time.sleep(3)\n",
    "            try:\n",
    "                time.sleep(1.7)\n",
    "                driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[2]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[2]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "                next_botton.click() \n",
    "\n",
    "                #Increment the page number\n",
    "                i += 1\n",
    "\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            time.sleep(2.5)\n",
    "                # check if current page is the last page. If true break away from the loop\n",
    "            time.sleep(3)\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "                next_botton.click() /html/body/div[6]/div[3]/div[2]/div/div/div[4]/div/div[2]/div/a/button\n",
    "\n",
    "                #Increment the page number\n",
    "                i += 1\n",
    "\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "\n",
    "            time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
