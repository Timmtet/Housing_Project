{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c755bf4",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> ON THE MARKET </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b5244",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> Sales </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d870e",
   "metadata": {},
   "source": [
    "First things first, we will import the required libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e418af4",
   "metadata": {},
   "source": [
    "Note: You have scrapped for Agents yet\n",
    "        The next button is not working yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0610b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8278900",
   "metadata": {},
   "source": [
    "Next, we will load in the dataset containing the list of postcode for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91de3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_csv('London postcode districts.xlsx - PC DIST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb8bdb",
   "metadata": {},
   "source": [
    "Now, we will create a function called 'OnTheMarket_sales' to scrap properties that are for sale on the OnTheMarket website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c2a39821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Find the search bar and enter the search query\n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(13)\n",
    "    #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    \n",
    "    # Scrapping data for the required features in the first page\n",
    "    time.sleep(2)\n",
    "    print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "    address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "    bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "    bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "    price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "    desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "    agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "    property_url_list = driver.current_url\n",
    "    Trans_type_list = Trans_type\n",
    "    website_list = website\n",
    "    for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "        address.append(address_item.text)\n",
    "        types.append(type_item.text)\n",
    "        bedrooms.append(bedroom_item.text)\n",
    "        bathrooms.append(bathroom_item.text)\n",
    "        prices.append(price_item.text)\n",
    "        desc.append(address_item.text + '. ' + type_item.text)\n",
    "        date_added.append(date_added_item.text)\n",
    "        agent_list.append(agent_list_item.text)\n",
    "        property_url.append(property_url_list)\n",
    "        Trans_type.append(Trans_type_list)\n",
    "        website.append(website_list)\n",
    "\n",
    "      \n",
    "    time.sleep(1.3)\n",
    "    # get the height of the page\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "    # scroll to the middle of the page using JavaScript\n",
    "    driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "    \n",
    "    # Click to cancel the pop-up window \n",
    "    time.sleep(1.5)\n",
    "    driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "    time.sleep(1.1) \n",
    "    next_botton = driver.find_element(By.XPATH, '//div/a/button') \n",
    "    \n",
    "    \n",
    "    last_botton_check = driver.find_element(By.XPATH, '//div/button') \n",
    "    html = last_botton_check.get_attribute('outerHTML') \n",
    "    if 'disabled' in html:\n",
    "        print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "    \n",
    "    time.sleep(1.3)\n",
    "    next_botton.click()\n",
    "    \n",
    "    # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=2\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        # check if current page is the last page. If true break away from the loop\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton.click()\n",
    "            \n",
    "            #Increment the page number\n",
    "            i += 1\n",
    "            \n",
    "        except:\n",
    "            print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "            break\n",
    "            \n",
    "        time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_sales(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2233b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1 from WC2B\n",
      "scraping page 2 from WC2B\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2B\n",
      "Total numbers of properties available in WC2B is 6\n",
      "scraping page 1 from WC2E\n",
      "scraping page 2 from WC2E\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2E\n",
      "Total numbers of properties available in WC2E is 6\n",
      "scraping page 1 from WC2H\n",
      "scraping page 2 from WC2H\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2H\n",
      "Total numbers of properties available in WC2H is 6\n",
      "scraping page 1 from WC2N\n",
      "scraping page 2 from WC2N\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2N\n",
      "Total numbers of properties available in WC2N is 8\n",
      "scraping page 1 from WC2R\n",
      "scraping page 2 from WC2R\n",
      "scraping page 3 from WC2R\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2R\n",
      "Total numbers of properties available in WC2R is 12\n",
      "------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS 38\n"
     ]
    }
   ],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes.loc[263:]['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_sales(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a11b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hi4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fc34a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postcode district</th>\n",
       "      <th>Local Areas</th>\n",
       "      <th>Borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>WC2B</td>\n",
       "      <td>Drury Lane, Aldwych</td>\n",
       "      <td>Camden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>WC2E</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>WC2H</td>\n",
       "      <td>Leicester Square</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>WC2N</td>\n",
       "      <td>Charing Cross</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>WC2R</td>\n",
       "      <td>Somerset House</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Postcode district          Local Areas      Borough\n",
       "263              WC2B  Drury Lane, Aldwych       Camden\n",
       "264              WC2E        Covent Garden  Westminster\n",
       "265              WC2H     Leicester Square  Westminster\n",
       "266              WC2N        Charing Cross  Westminster\n",
       "267              WC2R       Somerset House  Westminster"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = codes.loc[263:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a94c815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Find the search bar and enter the search query\n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(10)\n",
    "    #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "   \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    \n",
    "    # Scrapping data for the required features in the first page\n",
    "    time.sleep(2)\n",
    "    print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "    address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "    bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "    bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "    price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "    desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "    agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "    property_url_list = driver.current_url\n",
    "    Trans_type_list = Trans_type\n",
    "    website_list = website\n",
    "    for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "        address.append(address_item.text)\n",
    "        types.append(type_item.text)\n",
    "        bedrooms.append(bedroom_item.text)\n",
    "        bathrooms.append(bathroom_item.text)\n",
    "        prices.append(price_item.text)\n",
    "        desc.append(address_item.text + '. ' + type_item.text)\n",
    "        date_added.append(date_added_item.text)\n",
    "        agent_list.append(agent_list_item.text)\n",
    "        property_url.append(property_url_list)\n",
    "        Trans_type.append(Trans_type_list)\n",
    "        website.append(website_list)\n",
    "\n",
    "      \n",
    "    time.sleep(1.3)\n",
    "    # get the height of the page\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "    # scroll to the middle of the page using JavaScript\n",
    "    driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "    \n",
    "    # Click to cancel the pop-up window \n",
    "    time.sleep(1.5)\n",
    "    driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "    #time.sleep(1.1) \n",
    "    #next_botton = driver.find_element(By.XPATH, '//div/a/button') \n",
    "    \n",
    "    \n",
    "    #last_botton_check = driver.find_element(By.XPATH, '//div/button') \n",
    "    #html = last_botton_check.get_attribute('outerHTML') \n",
    "    #if 'disabled' in html:\n",
    "    print('Done')\n",
    "    \n",
    "    #time.sleep(1.3)\n",
    "    #next_botton.click()\n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=2\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        # check if current page is the last page. If true break away from the loop\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton.click()\n",
    "            \n",
    "            #Increment the page number\n",
    "            i += 1\n",
    "            \n",
    "        except:\n",
    "            print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "            break\n",
    "            \n",
    "        time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9f5bc",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> Rent </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "19cfc4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #click on Rent option \n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[1]/button[2]').click()\n",
    "    time.sleep(1.2)\n",
    "\n",
    "    # Find the search bar  \n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    time.sleep(1.3)\n",
    "    \n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(8) \n",
    "    driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    \n",
    "    # Scrapping data for the required features in the first page\n",
    "    time.sleep(2)\n",
    "    print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "    address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a') \n",
    "    type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\") \n",
    "    bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]') \n",
    "    bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]') \n",
    "    price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a') \n",
    "    desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "    agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "    property_url_list = driver.current_url\n",
    "    Trans_type_list = Trans_type\n",
    "    website_list = website\n",
    "    for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "        address.append(address_item.text)\n",
    "        types.append(type_item.text)\n",
    "        bedrooms.append(bedroom_item.text)\n",
    "        bathrooms.append(bathroom_item.text)\n",
    "        prices.append(price_item.text)\n",
    "        desc.append(address_item.text + '. ' + type_item.text)\n",
    "        date_added.append(date_added_item.text)\n",
    "        agent_list.append(agent_list_item.text)\n",
    "        property_url.append(property_url_list)\n",
    "        Trans_type.append(Trans_type_list)\n",
    "        website.append(website_list)\n",
    "\n",
    "      \n",
    "    time.sleep(1.3)\n",
    "    # get the height of the page\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "    # scroll to the middle of the page using JavaScript\n",
    "    driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "    \n",
    "    # Click to cancel the pop-up window \n",
    "    time.sleep(1.5)\n",
    "    driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "    time.sleep(1.1) \n",
    "    next_botton = driver.find_element(By.XPATH, '//div/a/button') \n",
    "    \n",
    "    \n",
    "    last_botton_check = driver.find_element(By.XPATH, '//div/button') \n",
    "    html = last_botton_check.get_attribute('outerHTML') \n",
    "    if 'disabled' in html:\n",
    "        print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "    \n",
    "    time.sleep(1.3)\n",
    "    next_botton.click()\n",
    "    \n",
    "    # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=2\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        # check if current page is the last page. If true break away from the loop\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton.click()\n",
    "            \n",
    "            #Increment the page number\n",
    "            i += 1\n",
    "            \n",
    "        except:\n",
    "            print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "            break\n",
    "            \n",
    "        time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Rent','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96d0ab90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1 from SE16\n",
      "scraping page 2 from SE16\n",
      "scraping page 3 from SE16\n",
      "scraping page 4 from SE16\n",
      "scraping page 5 from SE16\n",
      "------------------------------- SCRAPING COMPLETED FOR SE16\n",
      "Total numbers of properties available in SE16 is 109\n",
      "scraping page 1 from SE17\n",
      "scraping page 2 from SE17\n",
      "scraping page 3 from SE17\n",
      "scraping page 4 from SE17\n",
      "scraping page 5 from SE17\n",
      "------------------------------- SCRAPING COMPLETED FOR SE17\n",
      "Total numbers of properties available in SE17 is 96\n",
      "scraping page 1 from SE18\n",
      "scraping page 2 from SE18\n",
      "scraping page 3 from SE18\n",
      "scraping page 4 from SE18\n",
      "------------------------------- SCRAPING COMPLETED FOR SE18\n",
      "Total numbers of properties available in SE18 is 90\n",
      "scraping page 1 from SE19\n",
      "------------------------------- SCRAPING COMPLETED FOR SE19\n",
      "Total numbers of properties available in SE19 is 21\n",
      "scraping page 1 from SE2\n",
      "scraping page 2 from SE2\n",
      "------------------------------- SCRAPING COMPLETED FOR SE2\n",
      "Total numbers of properties available in SE2 is 38\n",
      "scraping page 1 from SE20\n",
      "------------------------------- SCRAPING COMPLETED FOR SE20\n",
      "Total numbers of properties available in SE20 is 22\n",
      "scraping page 1 from SE21\n",
      "------------------------------- SCRAPING COMPLETED FOR SE21\n",
      "Total numbers of properties available in SE21 is 24\n",
      "scraping page 1 from SE22\n",
      "------------------------------- SCRAPING COMPLETED FOR SE22\n",
      "Total numbers of properties available in SE22 is 19\n",
      "scraping page 1 from SE23\n",
      "------------------------------- SCRAPING COMPLETED FOR SE23\n",
      "Total numbers of properties available in SE23 is 22\n",
      "scraping page 1 from SE24\n",
      "------------------------------- SCRAPING COMPLETED FOR SE24\n",
      "Total numbers of properties available in SE24 is 19\n",
      "scraping page 1 from SE25\n",
      "scraping page 2 from SE25\n",
      "------------------------------- SCRAPING COMPLETED FOR SE25\n",
      "Total numbers of properties available in SE25 is 38\n",
      "scraping page 1 from SE26\n",
      "scraping page 2 from SE26\n",
      "------------------------------- SCRAPING COMPLETED FOR SE26\n",
      "Total numbers of properties available in SE26 is 32\n",
      "scraping page 1 from SE27\n",
      "------------------------------- SCRAPING COMPLETED FOR SE27\n",
      "Total numbers of properties available in SE27 is 14\n",
      "scraping page 1 from SE28\n",
      "------------------------------- SCRAPING COMPLETED FOR SE28\n",
      "Total numbers of properties available in SE28 is 24\n",
      "scraping page 1 from SE3\n",
      "scraping page 2 from SE3\n",
      "scraping page 3 from SE3\n",
      "------------------------------- SCRAPING COMPLETED FOR SE3\n",
      "Total numbers of properties available in SE3 is 53\n",
      "scraping page 1 from SE4\n",
      "scraping page 2 from SE4\n",
      "------------------------------- SCRAPING COMPLETED FOR SE4\n",
      "Total numbers of properties available in SE4 is 33\n",
      "scraping page 1 from SE5\n",
      "scraping page 2 from SE5\n",
      "------------------------------- SCRAPING COMPLETED FOR SE5\n",
      "Total numbers of properties available in SE5 is 38\n",
      "scraping page 1 from SE6\n",
      "scraping page 2 from SE6\n",
      "------------------------------- SCRAPING COMPLETED FOR SE6\n",
      "Total numbers of properties available in SE6 is 46\n",
      "scraping page 1 from SE7\n",
      "------------------------------- SCRAPING COMPLETED FOR SE7\n",
      "Total numbers of properties available in SE7 is 19\n",
      "scraping page 1 from SE8\n",
      "scraping page 2 from SE8\n",
      "------------------------------- SCRAPING COMPLETED FOR SE8\n",
      "Total numbers of properties available in SE8 is 44\n",
      "scraping page 1 from SE9\n",
      "------------------------------- SCRAPING COMPLETED FOR SE9\n",
      "Total numbers of properties available in SE9 is 23\n",
      "scraping page 1 from SM1\n",
      "scraping page 2 from SM1\n",
      "------------------------------- SCRAPING COMPLETED FOR SM1\n",
      "Total numbers of properties available in SM1 is 24\n",
      "scraping page 1 from SM2\n",
      "scraping page 2 from SM2\n",
      "------------------------------- SCRAPING COMPLETED FOR SM2\n",
      "Total numbers of properties available in SM2 is 36\n",
      "scraping page 1 from SM3\n",
      "------------------------------- SCRAPING COMPLETED FOR SM3\n",
      "Total numbers of properties available in SM3 is 25\n",
      "scraping page 1 from SM4\n",
      "------------------------------- SCRAPING COMPLETED FOR SM4\n",
      "Total numbers of properties available in SM4 is 16\n",
      "scraping page 1 from SM5\n",
      "------------------------------- SCRAPING COMPLETED FOR SM5\n",
      "Total numbers of properties available in SM5 is 24\n",
      "scraping page 1 from SM6\n",
      "------------------------------- SCRAPING COMPLETED FOR SM6\n",
      "Total numbers of properties available in SM6 is 16\n",
      "scraping page 1 from SM7\n",
      "------------------------------- SCRAPING COMPLETED FOR SM7\n",
      "Total numbers of properties available in SM7 is 7\n",
      "scraping page 1 from SW10\n",
      "scraping page 2 from SW10\n",
      "scraping page 3 from SW10\n",
      "scraping page 4 from SW10\n",
      "scraping page 5 from SW10\n",
      "scraping page 6 from SW10\n",
      "scraping page 7 from SW10\n",
      "scraping page 8 from SW10\n",
      "scraping page 9 from SW10\n",
      "------------------------------- SCRAPING COMPLETED FOR SW10\n",
      "Total numbers of properties available in SW10 is 209\n",
      "scraping page 1 from SW11\n",
      "scraping page 2 from SW11\n",
      "scraping page 3 from SW11\n",
      "scraping page 4 from SW11\n",
      "scraping page 5 from SW11\n",
      "scraping page 6 from SW11\n",
      "scraping page 7 from SW11\n",
      "scraping page 8 from SW11\n",
      "scraping page 9 from SW11\n",
      "scraping page 10 from SW11\n",
      "scraping page 11 from SW11\n",
      "scraping page 12 from SW11\n",
      "scraping page 13 from SW11\n",
      "scraping page 14 from SW11\n",
      "scraping page 15 from SW11\n",
      "scraping page 16 from SW11\n",
      "scraping page 17 from SW11\n",
      "scraping page 18 from SW11\n",
      "scraping page 19 from SW11\n",
      "scraping page 20 from SW11\n",
      "scraping page 21 from SW11\n",
      "scraping page 22 from SW11\n",
      "scraping page 23 from SW11\n",
      "scraping page 24 from SW11\n",
      "scraping page 25 from SW11\n",
      "scraping page 26 from SW11\n",
      "scraping page 27 from SW11\n",
      "scraping page 28 from SW11\n",
      "scraping page 29 from SW11\n",
      "scraping page 30 from SW11\n",
      "------------------------------- SCRAPING COMPLETED FOR SW11\n",
      "Total numbers of properties available in SW11 is 683\n",
      "scraping page 1 from SW12\n",
      "scraping page 2 from SW12\n",
      "------------------------------- SCRAPING COMPLETED FOR SW12\n",
      "Total numbers of properties available in SW12 is 46\n",
      "scraping page 1 from SW13\n",
      "scraping page 2 from SW13\n",
      "------------------------------- SCRAPING COMPLETED FOR SW13\n",
      "Total numbers of properties available in SW13 is 29\n",
      "scraping page 1 from SW14\n",
      "scraping page 2 from SW14\n",
      "------------------------------- SCRAPING COMPLETED FOR SW14\n",
      "Total numbers of properties available in SW14 is 28\n",
      "scraping page 1 from SW15\n",
      "scraping page 2 from SW15\n",
      "scraping page 3 from SW15\n",
      "scraping page 4 from SW15\n",
      "scraping page 5 from SW15\n",
      "scraping page 6 from SW15\n",
      "scraping page 7 from SW15\n",
      "scraping page 8 from SW15\n",
      "------------------------------- SCRAPING COMPLETED FOR SW15\n",
      "Total numbers of properties available in SW15 is 167\n",
      "scraping page 1 from SW16\n",
      "scraping page 2 from SW16\n",
      "scraping page 3 from SW16\n",
      "scraping page 4 from SW16\n",
      "scraping page 5 from SW16\n",
      "------------------------------- SCRAPING COMPLETED FOR SW16\n",
      "Total numbers of properties available in SW16 is 100\n",
      "scraping page 1 from SW17\n",
      "scraping page 2 from SW17\n",
      "scraping page 3 from SW17\n",
      "scraping page 4 from SW17\n",
      "scraping page 5 from SW17\n",
      "------------------------------- SCRAPING COMPLETED FOR SW17\n",
      "Total numbers of properties available in SW17 is 98\n",
      "scraping page 1 from SW18\n",
      "scraping page 2 from SW18\n",
      "scraping page 3 from SW18\n",
      "scraping page 4 from SW18\n",
      "scraping page 5 from SW18\n",
      "------------------------------- SCRAPING COMPLETED FOR SW18\n",
      "Total numbers of properties available in SW18 is 110\n",
      "scraping page 1 from SW19\n",
      "scraping page 2 from SW19\n",
      "scraping page 3 from SW19\n",
      "scraping page 4 from SW19\n",
      "scraping page 5 from SW19\n",
      "scraping page 6 from SW19\n",
      "scraping page 7 from SW19\n",
      "scraping page 8 from SW19\n",
      "scraping page 9 from SW19\n",
      "scraping page 10 from SW19\n",
      "scraping page 11 from SW19\n",
      "------------------------------- SCRAPING COMPLETED FOR SW19\n",
      "Total numbers of properties available in SW19 is 254\n",
      "scraping page 1 from SW1A\n",
      "scraping page 2 from SW1A\n",
      "------------------------------- SCRAPING COMPLETED FOR SW1A\n",
      "Total numbers of properties available in SW1A is 46\n",
      "scraping page 1 from SW1E\n",
      "------------------------------- SCRAPING COMPLETED FOR SW1E\n",
      "Total numbers of properties available in SW1E is 17\n",
      "scraping page 1 from SW1H\n",
      "------------------------------- SCRAPING COMPLETED FOR SW1H\n",
      "Total numbers of properties available in SW1H is 22\n",
      "scraping page 1 from SW1P\n",
      "scraping page 2 from SW1P\n",
      "scraping page 3 from SW1P\n",
      "scraping page 4 from SW1P\n",
      "scraping page 5 from SW1P\n",
      "scraping page 6 from SW1P\n",
      "scraping page 7 from SW1P\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------- SCRAPING COMPLETED FOR SW1P\n",
      "Total numbers of properties available in SW1P is 154\n",
      "scraping page 1 from SW1V\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# loop through postcodes\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m postcode \u001b[38;5;129;01min\u001b[39;00m codes\u001b[38;5;241m.\u001b[39mloc[\u001b[38;5;241m146\u001b[39m:][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPostcode district\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# call the function and pass the empty DataFrame as an argument\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m     df1 \u001b[38;5;241m=\u001b[39m \u001b[43mOnTheMarket_sales\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpostcode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSales\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mOnTheMarket\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m# append the df1 DataFrame to the empty DataFrame\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([df, df1], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[4], line 107\u001b[0m, in \u001b[0;36mOnTheMarket_sales\u001b[1;34m(postcodes, Trans_type, website, df)\u001b[0m\n\u001b[0;32m    104\u001b[0m driver\u001b[38;5;241m.\u001b[39mexecute_script(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwindow.scrollTo(0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage_height\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m1.37\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m);\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# Click to cancel the pop-up window \u001b[39;00m\n\u001b[1;32m--> 107\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.5\u001b[39m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    109\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.6\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes.loc[146:]['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_sales(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2b9af3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hey2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2400f58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postcode district</th>\n",
       "      <th>Local Areas</th>\n",
       "      <th>Borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>SE16</td>\n",
       "      <td>Rotherhithe, Bermondsey, Surrey Quays</td>\n",
       "      <td>Southwark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>SE17</td>\n",
       "      <td>Walworth, Newington</td>\n",
       "      <td>Southwark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>SE18</td>\n",
       "      <td>Plumstead, Woolwich</td>\n",
       "      <td>Greenwich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>SE19</td>\n",
       "      <td>Upper Norwood, Crystal Palace, Crown Point, No...</td>\n",
       "      <td>Croydon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>SE2</td>\n",
       "      <td>Abbey Wood, West Heath, Crossness, Thamesmead</td>\n",
       "      <td>Greenwich</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>WC2B</td>\n",
       "      <td>Drury Lane, Aldwych</td>\n",
       "      <td>Camden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>WC2E</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>WC2H</td>\n",
       "      <td>Leicester Square</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>WC2N</td>\n",
       "      <td>Charing Cross</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>WC2R</td>\n",
       "      <td>Somerset House</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Postcode district                                        Local Areas   \n",
       "146              SE16              Rotherhithe, Bermondsey, Surrey Quays  \\\n",
       "147              SE17                                Walworth, Newington   \n",
       "148              SE18                                Plumstead, Woolwich   \n",
       "149              SE19  Upper Norwood, Crystal Palace, Crown Point, No...   \n",
       "150               SE2      Abbey Wood, West Heath, Crossness, Thamesmead   \n",
       "..                ...                                                ...   \n",
       "263              WC2B                                Drury Lane, Aldwych   \n",
       "264              WC2E                                      Covent Garden   \n",
       "265              WC2H                                   Leicester Square   \n",
       "266              WC2N                                      Charing Cross   \n",
       "267              WC2R                                     Somerset House   \n",
       "\n",
       "         Borough  \n",
       "146    Southwark  \n",
       "147    Southwark  \n",
       "148    Greenwich  \n",
       "149      Croydon  \n",
       "150    Greenwich  \n",
       "..           ...  \n",
       "263       Camden  \n",
       "264  Westminster  \n",
       "265  Westminster  \n",
       "266  Westminster  \n",
       "267  Westminster  \n",
       "\n",
       "[122 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = codes.loc[146:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "639a6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #click on Rent option \n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[1]/button[2]').click()\n",
    "    time.sleep(1.2)\n",
    "\n",
    "    # Find the search bar  \n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    time.sleep(1.3)\n",
    "    \n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(10)\n",
    "    #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "   \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    \n",
    "    # Scrapping data for the required features in the first page\n",
    "    time.sleep(2)\n",
    "    print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "    address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "    bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "    bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "    price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a')\n",
    "    desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "    date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "    agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "    property_url_list = driver.current_url\n",
    "    Trans_type_list = Trans_type\n",
    "    website_list = website\n",
    "    for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "        address.append(address_item.text)\n",
    "        types.append(type_item.text)\n",
    "        bedrooms.append(bedroom_item.text)\n",
    "        bathrooms.append(bathroom_item.text)\n",
    "        prices.append(price_item.text)\n",
    "        desc.append(address_item.text + '. ' + type_item.text)\n",
    "        date_added.append(date_added_item.text)\n",
    "        agent_list.append(agent_list_item.text)\n",
    "        property_url.append(property_url_list)\n",
    "        Trans_type.append(Trans_type_list)\n",
    "        website.append(website_list)\n",
    "\n",
    "      \n",
    "    time.sleep(1.3)\n",
    "    # get the height of the page\n",
    "    page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "    # scroll to the middle of the page using JavaScript\n",
    "    driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "    \n",
    "    # Click to cancel the pop-up window \n",
    "    time.sleep(1.5)\n",
    "    driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "    #time.sleep(1.1) \n",
    "    #next_botton = driver.find_element(By.XPATH, '//div/a/button') \n",
    "    \n",
    "    \n",
    "    #last_botton_check = driver.find_element(By.XPATH, '//div/button') \n",
    "    #html = last_botton_check.get_attribute('outerHTML') \n",
    "    #if 'disabled' in html:\n",
    "    print('Done')\n",
    "    \n",
    "    #time.sleep(1.3)\n",
    "    #next_botton.click()\n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Rent','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=2\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        # check if current page is the last page. If true break away from the loop\n",
    "        time.sleep(3)\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            next_botton.click()\n",
    "            \n",
    "            #Increment the page number\n",
    "            i += 1\n",
    "            \n",
    "        except:\n",
    "            print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "            break\n",
    "            \n",
    "        time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n",
    "    '''"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e78c987e",
   "metadata": {},
   "source": [
    "This is the master script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0076e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #click on Rent option \n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[1]/button[2]').click()\n",
    "    time.sleep(1.2)\n",
    "\n",
    "    # Find the search bar  \n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    time.sleep(1.3)\n",
    "    \n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(13) \n",
    "    try:\n",
    "        driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    except:\n",
    "        driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[2]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    while True:\n",
    "        # Scrapping data for the required features in the first page\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "\n",
    "        time.sleep(1.3)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//1.37});\")\n",
    "\n",
    "        # Click to cancel the pop-up window \n",
    "        time.sleep(1.5)\n",
    "        try:\n",
    "            time.sleep(1.6)\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button')\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "            time.sleep(1.1)\n",
    "        except:\n",
    "            time.sleep(1.5)\n",
    "\n",
    "\n",
    "        # Find the next button\n",
    "        if i==1:\n",
    "            try:\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "\n",
    "                # Find the last next button \n",
    "                last_botton_check = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "        else:\n",
    "            try:\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "\n",
    "                # Find the last next button \n",
    "                last_botton_check = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "            \n",
    "        #Click the next button if it's not the last \n",
    "        time.sleep(5.5)\n",
    "        #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "        next_botton.click()\n",
    "        \n",
    "        #Increment the page number\n",
    "        i += 1\n",
    "        time.sleep(2)\n",
    "                \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Rent','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058ac42e",
   "metadata": {},
   "outputs": [],
   "source": [
    "<button class=\"otm-Button whitespace-nowrap py-2 leading-normal h-auto\n",
    "shadow-none font-heading font-semibold text-center justify-center inli\n",
    "ne-flex items-center px-8 rounded-md border-2 disabled:bg-dove disabled\n",
    "    :text-white disabled:border-transparent hover:disabled:opacity-100 \n",
    "                    transition duration-200 ease-in-out border-burnt-\n",
    "                    coral text-burnt-coral hover:opacity-70  min-h-[\n",
    "52px] text-md md:min-h-[70px] sm:text-base2\">Next</button>\n",
    "                                \n",
    "                                \n",
    "<button class=\"otm-Button whitespace-nowrap py-2 leading-normal h-auto\n",
    "shadow-none font-heading font-semibold text-center justify-center inli\n",
    "ne-flex items-center px-8 rounded-md border-2 disabled:bg-dove disabled\n",
    "    :text-white disabled:border-transparent hover:disabled:opacity-100\n",
    "                    transition duration-200 ease-in-out border-burnt-\n",
    "                    coral text-burnt-coral hover:opacity-70 order-last min-h-[52px]\n",
    "text-md md:min-h-[70px] sm:text-base2\" disabled=\"\">Next</button>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7141f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Find the search bar and enter the search query\n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(10)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "   \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    \n",
    "     # Setting the page number to be 2 and looping over the available pages for each postcode\n",
    "    i=1\n",
    "    while True:\n",
    "        # Scrapping data for the required features\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + ' ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "        \n",
    "        time.sleep(2.5)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//3});\")\n",
    "        \n",
    "        if i == 1:\n",
    "             # Click to cancel the pop-up window \n",
    "            time.sleep(1.5)\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "            time.sleep(3)\n",
    "            try:\n",
    "                time.sleep(1.7)\n",
    "                driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[2]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[2]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "                next_botton.click() \n",
    "\n",
    "                #Increment the page number\n",
    "                i += 1\n",
    "\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "            \n",
    "        \n",
    "        else:\n",
    "            time.sleep(2.5)\n",
    "                # check if current page is the last page. If true break away from the loop\n",
    "            time.sleep(3)\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "                next_botton.click() /html/body/div[6]/div[3]/div[2]/div/div/div[4]/div/div[2]/div/a/button\n",
    "\n",
    "                #Increment the page number\n",
    "                i += 1\n",
    "\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "\n",
    "            time.sleep(1.3) \n",
    "        \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
