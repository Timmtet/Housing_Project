{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c755bf4",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> ON THE MARKET </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b5244",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> Sales </h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045d870e",
   "metadata": {},
   "source": [
    "First things first, we will import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0610b43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8278900",
   "metadata": {},
   "source": [
    "Next, we will load in the dataset containing the list of postcode for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91de3d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "codes = pd.read_csv('London postcode districts.xlsx - PC DIST.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acb8bdb",
   "metadata": {},
   "source": [
    "Now, we will create a function called 'OnTheMarket_sales' to scrap properties that are for sale on the OnTheMarket website"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6313b6",
   "metadata": {},
   "source": [
    "# Recheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8d8f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_sales(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "\n",
    "    # Find the search bar  \n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    time.sleep(1.3)\n",
    "    \n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(13) \n",
    "    try:\n",
    "        driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    except:\n",
    "        driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[2]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    while True:\n",
    "        # Scrapping data for the required features in the first page\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list =driver.find_elements(By.XPATH, '//div[2]/div[2]/a[2]')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "\n",
    "        time.sleep(1.3)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//1.37});\")\n",
    "\n",
    "        # Click to cancel the pop-up window \n",
    "        time.sleep(1.5)\n",
    "        try:\n",
    "            time.sleep(1.6)\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button')\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "            time.sleep(1.1)\n",
    "        except:\n",
    "            time.sleep(1.5)\n",
    "\n",
    "\n",
    "        # Find the next button\n",
    "        if i==1:\n",
    "            try:\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "                                                            \n",
    "                # Find the last next button \n",
    "                last_botton_check = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "        else:\n",
    "            try:\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "\n",
    "                # Find the last next button \n",
    "                last_botton_check = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "            \n",
    "        #Click the next button if it's not the last \n",
    "        time.sleep(5.5)\n",
    "        #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "        next_botton.click()\n",
    "        \n",
    "        #Increment the page number\n",
    "        i += 1\n",
    "        time.sleep(2)\n",
    "                \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Sales','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc0a10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_sales(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2233b62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1 from WC2B\n",
      "scraping page 2 from WC2B\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2B\n",
      "Total numbers of properties available in WC2B is 6\n",
      "scraping page 1 from WC2E\n",
      "scraping page 2 from WC2E\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2E\n",
      "Total numbers of properties available in WC2E is 6\n",
      "scraping page 1 from WC2H\n",
      "scraping page 2 from WC2H\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2H\n",
      "Total numbers of properties available in WC2H is 6\n",
      "scraping page 1 from WC2N\n",
      "scraping page 2 from WC2N\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2N\n",
      "Total numbers of properties available in WC2N is 8\n",
      "scraping page 1 from WC2R\n",
      "scraping page 2 from WC2R\n",
      "scraping page 3 from WC2R\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2R\n",
      "Total numbers of properties available in WC2R is 12\n",
      "------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS 38\n"
     ]
    }
   ],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes.loc[263:]['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_sales(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6a11b676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('hi4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3fc34a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postcode district</th>\n",
       "      <th>Local Areas</th>\n",
       "      <th>Borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>WC2B</td>\n",
       "      <td>Drury Lane, Aldwych</td>\n",
       "      <td>Camden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>WC2E</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>WC2H</td>\n",
       "      <td>Leicester Square</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>WC2N</td>\n",
       "      <td>Charing Cross</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>WC2R</td>\n",
       "      <td>Somerset House</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Postcode district          Local Areas      Borough\n",
       "263              WC2B  Drury Lane, Aldwych       Camden\n",
       "264              WC2E        Covent Garden  Westminster\n",
       "265              WC2H     Leicester Square  Westminster\n",
       "266              WC2N        Charing Cross  Westminster\n",
       "267              WC2R       Somerset House  Westminster"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = codes.loc[263:]\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c9f5bc",
   "metadata": {},
   "source": [
    "<h1 style='background-color: Purple; padding: 10px; color: white'> Rent </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de56026b",
   "metadata": {},
   "source": [
    "Now, we will create a function called 'OnTheMarket_rent' to scrap properties that are for rent on the OnTheMarket website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0076e00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def OnTheMarket_rent(postcodes, Trans_type, website, df):\n",
    "    \"\"\"\n",
    "    This function scrapes property data from Rightmove for a given list of postcodes and transaction type,\n",
    "    and returns the data as a pandas DataFrame.\n",
    "\n",
    "    Arguments:\n",
    "    postcodes -- a list of postcodes for which to scrape property data\n",
    "    Trans_type -- the transaction type of the properties to be scraped ('sales' or 'rent')\n",
    "    website -- the name of the website being scraped (in this case, 'Rightmove')\n",
    "    df -- an empty pandas DataFrame to store the scraped data\n",
    "\n",
    "    Returns:\n",
    "    df1 -- a pandas DataFrame containing the scraped property data\n",
    "    \n",
    "    \"\"\"    \n",
    "    # Set the path to the ChromeDriver executable\n",
    "    s = Service(\"C:\\\\Users\\\\user\\\\Downloads\\\\Set ups\\\\chromedriver_win32\\\\chromedriver.exe\")\n",
    "    \n",
    "    # Launch the ChromeDriver with the specified service\n",
    "    driver = webdriver.Chrome(service= s)\n",
    "    \n",
    "    # Navigate to the specified URL \n",
    "    driver.get('https://www.onthemarket.com/') \n",
    "\n",
    "    # Click to cancel the pop-up window and maximize the window\n",
    "    time.sleep(3.3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[1]/div/div/div[2]/button').click()\n",
    "    time.sleep(2)\n",
    "    driver.maximize_window()\n",
    "    \n",
    "    #click on Rent option \n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[1]/button[2]').click()\n",
    "    time.sleep(1.2)\n",
    "\n",
    "    # Find the search bar  \n",
    "    time.sleep(1.2)\n",
    "    search = driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/input')\n",
    "    search.send_keys(postcode)\n",
    "    time.sleep(1.3)\n",
    "    \n",
    "    \n",
    "    # Click the search button\n",
    "    time.sleep(3)\n",
    "    driver.find_element(By.XPATH, '/html/body/div[2]/div[3]/div/div/div/div/div[2]/div/div[1]/div/div/button').click()\n",
    "    \n",
    "    # Enable list view\n",
    "    time.sleep(13) \n",
    "    try:\n",
    "        driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    except:\n",
    "        driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[2]/div/div/div[2]/div/div[3]/div[1]/span[2]/a/span').click()\n",
    "    \n",
    "    # Initialize empty lists for storing scraped data\n",
    "    Trans_type = []\n",
    "    address = []\n",
    "    types = []\n",
    "    bedrooms = []\n",
    "    bathrooms = []\n",
    "    prices = []\n",
    "    desc = []\n",
    "    date_added = []\n",
    "    agent_list = []\n",
    "    property_url = []\n",
    "    website = []\n",
    "    \n",
    "    # Create an empty dataframe to store the scraped data\n",
    "    df = pd.DataFrame()\n",
    "    \n",
    "     # Setting the page number to be 1 \n",
    "    i = 1\n",
    "    while True:\n",
    "        # Scrapping data for the required features in the first page\n",
    "        time.sleep(2)\n",
    "        print(\"{} {} {} {}\".format('scraping page', i,'from', postcode ))\n",
    "        address_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        type_list = driver.find_elements(By.XPATH, \"//div[2]/p/span[1]/a\")\n",
    "        bedroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[1]')\n",
    "        bathroom_list = driver.find_elements(By.XPATH, '//div[2]/div[4]/div[2]')\n",
    "        price_list = driver.find_elements(By.XPATH, '//div[2]/div[2]/a')\n",
    "        desc_list = driver.find_elements(By.XPATH, '//div[2]/p/span[2]/a')\n",
    "        date_added_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/a')\n",
    "        agent_list_list = driver.find_elements(By.XPATH, '//div[3]/div[1]/div[2]/small/a')\n",
    "        property_url_list = driver.current_url\n",
    "        Trans_type_list = Trans_type\n",
    "        website_list = website\n",
    "        for address_item, type_item, bedroom_item, bathroom_item, price_item, desc_item, date_added_item, agent_list_item in zip(address_list, type_list, bedroom_list, bathroom_list, price_list, desc_list, date_added_list, agent_list_list):\n",
    "            address.append(address_item.text)\n",
    "            types.append(type_item.text)\n",
    "            bedrooms.append(bedroom_item.text)\n",
    "            bathrooms.append(bathroom_item.text)\n",
    "            prices.append(price_item.text)\n",
    "            desc.append(address_item.text + '. ' + type_item.text)\n",
    "            date_added.append(date_added_item.text)\n",
    "            agent_list.append(agent_list_item.text)\n",
    "            property_url.append(property_url_list)\n",
    "            Trans_type.append(Trans_type_list)\n",
    "            website.append(website_list)\n",
    "\n",
    "\n",
    "        time.sleep(1.3)\n",
    "        # get the height of the page\n",
    "        page_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "        # scroll to the middle of the page using JavaScript\n",
    "        driver.execute_script(f\"window.scrollTo(0, {page_height//1.37});\")\n",
    "\n",
    "        # Click to cancel the pop-up window \n",
    "        time.sleep(1.5)\n",
    "        try:\n",
    "            time.sleep(1.6)\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button')\n",
    "            driver.find_element(By.XPATH, '//div[5]/div/div/button').click()\n",
    "            time.sleep(1.1)\n",
    "        except:\n",
    "            time.sleep(1.5)\n",
    "\n",
    "\n",
    "        # Find the next button\n",
    "        if i==1:\n",
    "            try:\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "\n",
    "                # Find the last next button \n",
    "                last_botton_check = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "        else:\n",
    "            try:\n",
    "                next_botton = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "\n",
    "                # Find the last next button \n",
    "                last_botton_check = driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a[2]/button')\n",
    "            except:\n",
    "                print('------------------------------- SCRAPING COMPLETED FOR ' + postcode)\n",
    "                break\n",
    "            \n",
    "        #Click the next button if it's not the last \n",
    "        time.sleep(5.5)\n",
    "        #driver.find_element(By.XPATH, '/html/body/div[6]/div[3]/div[3]/div/div/div[4]/div/div[2]/div/a/button')\n",
    "        next_botton.click()\n",
    "        \n",
    "        #Increment the page number\n",
    "        i += 1\n",
    "        time.sleep(2)\n",
    "                \n",
    "        \n",
    "    # Create a dataframe to store data scrapped for each postcode    \n",
    "    df1 = pd.DataFrame({'Location': postcode, 'Tansaction_Type': 'Rent','Property_Type':types, 'Address' :address, 'Bedrooms': bedrooms, 'Bathrooms':bathrooms, 'Price':prices, 'Description': desc, 'Listing_Date':date_added, 'Agent':agent_list, 'Listing_Source': 'OnTheMartket', 'listing_URL':property_url})\n",
    "    df1.insert(0, 'Unique_Id', [f'{postcode}S{i+1:05d}OM' for i in range(len(df1))])\n",
    "    print('Total numbers of properties available in ' + postcode + ' is ' + str(df1.shape[0]))\n",
    "\n",
    "     # Concat the dataframe obtain for all postcodes\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "    \n",
    "     # Return a dataframe\n",
    "    return df1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96d0ab90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1 from SW1V\n",
      "scraping page 2 from SW1V\n",
      "scraping page 3 from SW1V\n",
      "scraping page 4 from SW1V\n",
      "scraping page 5 from SW1V\n",
      "scraping page 6 from SW1V\n",
      "------------------------------- SCRAPING COMPLETED FOR SW1V\n",
      "Total numbers of properties available in SW1V is 117\n",
      "scraping page 1 from SW1W\n",
      "scraping page 2 from SW1W\n",
      "scraping page 3 from SW1W\n",
      "scraping page 4 from SW1W\n",
      "scraping page 5 from SW1W\n",
      "scraping page 6 from SW1W\n",
      "scraping page 7 from SW1W\n",
      "scraping page 8 from SW1W\n",
      "scraping page 9 from SW1W\n",
      "scraping page 10 from SW1W\n",
      "------------------------------- SCRAPING COMPLETED FOR SW1W\n",
      "Total numbers of properties available in SW1W is 222\n",
      "scraping page 1 from SW1X\n",
      "scraping page 2 from SW1X\n",
      "scraping page 3 from SW1X\n",
      "scraping page 4 from SW1X\n",
      "scraping page 5 from SW1X\n",
      "scraping page 6 from SW1X\n",
      "scraping page 7 from SW1X\n",
      "scraping page 8 from SW1X\n",
      "scraping page 9 from SW1X\n",
      "scraping page 10 from SW1X\n",
      "------------------------------- SCRAPING COMPLETED FOR SW1X\n",
      "Total numbers of properties available in SW1X is 219\n",
      "scraping page 1 from SW1Y\n",
      "scraping page 2 from SW1Y\n",
      "------------------------------- SCRAPING COMPLETED FOR SW1Y\n",
      "Total numbers of properties available in SW1Y is 46\n",
      "scraping page 1 from SW2\n",
      "scraping page 2 from SW2\n",
      "------------------------------- SCRAPING COMPLETED FOR SW2\n",
      "Total numbers of properties available in SW2 is 46\n",
      "scraping page 1 from SW20\n",
      "scraping page 2 from SW20\n",
      "scraping page 3 from SW20\n",
      "------------------------------- SCRAPING COMPLETED FOR SW20\n",
      "Total numbers of properties available in SW20 is 56\n",
      "scraping page 1 from SW3\n",
      "scraping page 2 from SW3\n",
      "scraping page 3 from SW3\n",
      "scraping page 4 from SW3\n",
      "scraping page 5 from SW3\n",
      "scraping page 6 from SW3\n",
      "scraping page 7 from SW3\n",
      "scraping page 8 from SW3\n",
      "scraping page 9 from SW3\n",
      "scraping page 10 from SW3\n",
      "scraping page 11 from SW3\n",
      "scraping page 12 from SW3\n",
      "scraping page 13 from SW3\n",
      "scraping page 14 from SW3\n",
      "scraping page 15 from SW3\n",
      "scraping page 16 from SW3\n",
      "scraping page 17 from SW3\n",
      "scraping page 18 from SW3\n",
      "scraping page 19 from SW3\n",
      "scraping page 20 from SW3\n",
      "scraping page 21 from SW3\n",
      "scraping page 22 from SW3\n",
      "scraping page 23 from SW3\n",
      "scraping page 24 from SW3\n",
      "scraping page 25 from SW3\n",
      "scraping page 26 from SW3\n",
      "scraping page 27 from SW3\n",
      "scraping page 28 from SW3\n",
      "------------------------------- SCRAPING COMPLETED FOR SW3\n",
      "Total numbers of properties available in SW3 is 646\n",
      "scraping page 1 from SW4\n",
      "scraping page 2 from SW4\n",
      "scraping page 3 from SW4\n",
      "scraping page 4 from SW4\n",
      "------------------------------- SCRAPING COMPLETED FOR SW4\n",
      "Total numbers of properties available in SW4 is 80\n",
      "scraping page 1 from SW5\n",
      "scraping page 2 from SW5\n",
      "scraping page 3 from SW5\n",
      "scraping page 4 from SW5\n",
      "scraping page 5 from SW5\n",
      "scraping page 6 from SW5\n",
      "scraping page 7 from SW5\n",
      "scraping page 8 from SW5\n",
      "------------------------------- SCRAPING COMPLETED FOR SW5\n",
      "Total numbers of properties available in SW5 is 175\n",
      "scraping page 1 from SW6\n",
      "scraping page 2 from SW6\n",
      "scraping page 3 from SW6\n",
      "scraping page 4 from SW6\n",
      "scraping page 5 from SW6\n",
      "scraping page 6 from SW6\n",
      "scraping page 7 from SW6\n",
      "scraping page 8 from SW6\n",
      "scraping page 9 from SW6\n",
      "scraping page 10 from SW6\n",
      "scraping page 11 from SW6\n",
      "scraping page 12 from SW6\n",
      "scraping page 13 from SW6\n",
      "scraping page 14 from SW6\n",
      "scraping page 15 from SW6\n",
      "scraping page 16 from SW6\n",
      "scraping page 17 from SW6\n",
      "scraping page 18 from SW6\n",
      "------------------------------- SCRAPING COMPLETED FOR SW6\n",
      "Total numbers of properties available in SW6 is 412\n",
      "scraping page 1 from SW7\n",
      "scraping page 2 from SW7\n",
      "scraping page 3 from SW7\n",
      "scraping page 4 from SW7\n",
      "scraping page 5 from SW7\n",
      "scraping page 6 from SW7\n",
      "scraping page 7 from SW7\n",
      "scraping page 8 from SW7\n",
      "scraping page 9 from SW7\n",
      "scraping page 10 from SW7\n",
      "scraping page 11 from SW7\n",
      "scraping page 12 from SW7\n",
      "scraping page 13 from SW7\n",
      "scraping page 14 from SW7\n",
      "scraping page 15 from SW7\n",
      "scraping page 16 from SW7\n",
      "scraping page 17 from SW7\n",
      "scraping page 18 from SW7\n",
      "scraping page 19 from SW7\n",
      "scraping page 20 from SW7\n",
      "scraping page 21 from SW7\n",
      "scraping page 22 from SW7\n",
      "scraping page 23 from SW7\n",
      "scraping page 24 from SW7\n",
      "scraping page 25 from SW7\n",
      "scraping page 26 from SW7\n",
      "scraping page 27 from SW7\n",
      "scraping page 28 from SW7\n",
      "scraping page 29 from SW7\n",
      "scraping page 30 from SW7\n",
      "scraping page 31 from SW7\n",
      "scraping page 32 from SW7\n",
      "scraping page 33 from SW7\n",
      "------------------------------- SCRAPING COMPLETED FOR SW7\n",
      "Total numbers of properties available in SW7 is 757\n",
      "scraping page 1 from SW8\n",
      "scraping page 2 from SW8\n",
      "scraping page 3 from SW8\n",
      "scraping page 4 from SW8\n",
      "scraping page 5 from SW8\n",
      "scraping page 6 from SW8\n",
      "scraping page 7 from SW8\n",
      "scraping page 8 from SW8\n",
      "scraping page 9 from SW8\n",
      "scraping page 10 from SW8\n",
      "scraping page 11 from SW8\n",
      "scraping page 12 from SW8\n",
      "scraping page 13 from SW8\n",
      "scraping page 14 from SW8\n",
      "scraping page 15 from SW8\n",
      "scraping page 16 from SW8\n",
      "scraping page 17 from SW8\n",
      "scraping page 18 from SW8\n",
      "scraping page 19 from SW8\n",
      "scraping page 20 from SW8\n",
      "scraping page 21 from SW8\n",
      "------------------------------- SCRAPING COMPLETED FOR SW8\n",
      "Total numbers of properties available in SW8 is 481\n",
      "scraping page 1 from SW9\n",
      "scraping page 2 from SW9\n",
      "scraping page 3 from SW9\n",
      "------------------------------- SCRAPING COMPLETED FOR SW9\n",
      "Total numbers of properties available in SW9 is 71\n",
      "scraping page 1 from TN14\n",
      "------------------------------- SCRAPING COMPLETED FOR TN14\n",
      "Total numbers of properties available in TN14 is 24\n",
      "scraping page 1 from TN16\n",
      "------------------------------- SCRAPING COMPLETED FOR TN16\n",
      "Total numbers of properties available in TN16 is 18\n",
      "scraping page 1 from TW1\n",
      "------------------------------- SCRAPING COMPLETED FOR TW1\n",
      "Total numbers of properties available in TW1 is 18\n",
      "scraping page 1 from TW10\n",
      "scraping page 2 from TW10\n",
      "scraping page 3 from TW10\n",
      "------------------------------- SCRAPING COMPLETED FOR TW10\n",
      "Total numbers of properties available in TW10 is 59\n",
      "scraping page 1 from TW11\n",
      "scraping page 2 from TW11\n",
      "------------------------------- SCRAPING COMPLETED FOR TW11\n",
      "Total numbers of properties available in TW11 is 34\n",
      "scraping page 1 from TW12\n",
      "------------------------------- SCRAPING COMPLETED FOR TW12\n",
      "Total numbers of properties available in TW12 is 22\n",
      "scraping page 1 from TW13\n",
      "------------------------------- SCRAPING COMPLETED FOR TW13\n",
      "Total numbers of properties available in TW13 is 18\n",
      "scraping page 1 from TW14\n",
      "------------------------------- SCRAPING COMPLETED FOR TW14\n",
      "Total numbers of properties available in TW14 is 23\n",
      "scraping page 1 from TW16\n",
      "------------------------------- SCRAPING COMPLETED FOR TW16\n",
      "Total numbers of properties available in TW16 is 13\n",
      "scraping page 1 from TW19\n",
      "------------------------------- SCRAPING COMPLETED FOR TW19\n",
      "Total numbers of properties available in TW19 is 16\n",
      "scraping page 1 from TW2\n",
      "------------------------------- SCRAPING COMPLETED FOR TW2\n",
      "Total numbers of properties available in TW2 is 12\n",
      "scraping page 1 from TW3\n",
      "scraping page 2 from TW3\n",
      "------------------------------- SCRAPING COMPLETED FOR TW3\n",
      "Total numbers of properties available in TW3 is 42\n",
      "scraping page 1 from TW4\n",
      "scraping page 2 from TW4\n",
      "------------------------------- SCRAPING COMPLETED FOR TW4\n",
      "Total numbers of properties available in TW4 is 27\n",
      "scraping page 1 from TW5\n",
      "scraping page 2 from TW5\n",
      "------------------------------- SCRAPING COMPLETED FOR TW5\n",
      "Total numbers of properties available in TW5 is 33\n",
      "scraping page 1 from TW6\n",
      "------------------------------- SCRAPING COMPLETED FOR TW6\n",
      "Total numbers of properties available in TW6 is 24\n",
      "scraping page 1 from TW7\n",
      "scraping page 2 from TW7\n",
      "------------------------------- SCRAPING COMPLETED FOR TW7\n",
      "Total numbers of properties available in TW7 is 35\n",
      "scraping page 1 from TW8\n",
      "scraping page 2 from TW8\n",
      "scraping page 3 from TW8\n",
      "scraping page 4 from TW8\n",
      "------------------------------- SCRAPING COMPLETED FOR TW8\n",
      "Total numbers of properties available in TW8 is 88\n",
      "scraping page 1 from TW9\n",
      "scraping page 2 from TW9\n",
      "scraping page 3 from TW9\n",
      "------------------------------- SCRAPING COMPLETED FOR TW9\n",
      "Total numbers of properties available in TW9 is 48\n",
      "scraping page 1 from UB1\n",
      "------------------------------- SCRAPING COMPLETED FOR UB1\n",
      "Total numbers of properties available in UB1 is 21\n",
      "scraping page 1 from UB10\n",
      "------------------------------- SCRAPING COMPLETED FOR UB10\n",
      "Total numbers of properties available in UB10 is 21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 1 from UB11\n",
      "------------------------------- SCRAPING COMPLETED FOR UB11\n",
      "Total numbers of properties available in UB11 is 7\n",
      "scraping page 1 from UB2\n",
      "scraping page 2 from UB2\n",
      "------------------------------- SCRAPING COMPLETED FOR UB2\n",
      "Total numbers of properties available in UB2 is 41\n",
      "scraping page 1 from UB3\n",
      "scraping page 2 from UB3\n",
      "------------------------------- SCRAPING COMPLETED FOR UB3\n",
      "Total numbers of properties available in UB3 is 27\n",
      "scraping page 1 from UB4\n",
      "------------------------------- SCRAPING COMPLETED FOR UB4\n",
      "Total numbers of properties available in UB4 is 12\n",
      "scraping page 1 from UB5\n",
      "------------------------------- SCRAPING COMPLETED FOR UB5\n",
      "Total numbers of properties available in UB5 is 16\n",
      "scraping page 1 from UB6\n",
      "scraping page 2 from UB6\n",
      "scraping page 3 from UB6\n",
      "------------------------------- SCRAPING COMPLETED FOR UB6\n",
      "Total numbers of properties available in UB6 is 52\n",
      "scraping page 1 from UB7\n",
      "------------------------------- SCRAPING COMPLETED FOR UB7\n",
      "Total numbers of properties available in UB7 is 16\n",
      "scraping page 1 from UB8\n",
      "scraping page 2 from UB8\n",
      "scraping page 3 from UB8\n",
      "------------------------------- SCRAPING COMPLETED FOR UB8\n",
      "Total numbers of properties available in UB8 is 65\n",
      "scraping page 1 from UB9\n",
      "------------------------------- SCRAPING COMPLETED FOR UB9\n",
      "Total numbers of properties available in UB9 is 14\n",
      "scraping page 1 from W10\n",
      "scraping page 2 from W10\n",
      "scraping page 3 from W10\n",
      "------------------------------- SCRAPING COMPLETED FOR W10\n",
      "Total numbers of properties available in W10 is 63\n",
      "scraping page 1 from W11\n",
      "scraping page 2 from W11\n",
      "scraping page 3 from W11\n",
      "scraping page 4 from W11\n",
      "scraping page 5 from W11\n",
      "scraping page 6 from W11\n",
      "scraping page 7 from W11\n",
      "scraping page 8 from W11\n",
      "------------------------------- SCRAPING COMPLETED FOR W11\n",
      "Total numbers of properties available in W11 is 178\n",
      "scraping page 1 from W12\n",
      "scraping page 2 from W12\n",
      "scraping page 3 from W12\n",
      "scraping page 4 from W12\n",
      "scraping page 5 from W12\n",
      "scraping page 6 from W12\n",
      "scraping page 7 from W12\n",
      "------------------------------- SCRAPING COMPLETED FOR W12\n",
      "Total numbers of properties available in W12 is 158\n",
      "scraping page 1 from W13\n",
      "scraping page 2 from W13\n",
      "------------------------------- SCRAPING COMPLETED FOR W13\n",
      "Total numbers of properties available in W13 is 46\n",
      "scraping page 1 from W14\n",
      "scraping page 2 from W14\n",
      "scraping page 3 from W14\n",
      "scraping page 4 from W14\n",
      "scraping page 5 from W14\n",
      "scraping page 6 from W14\n",
      "scraping page 7 from W14\n",
      "scraping page 8 from W14\n",
      "scraping page 9 from W14\n",
      "scraping page 10 from W14\n",
      "scraping page 11 from W14\n",
      "------------------------------- SCRAPING COMPLETED FOR W14\n",
      "Total numbers of properties available in W14 is 253\n",
      "scraping page 1 from W1B\n",
      "scraping page 2 from W1B\n",
      "scraping page 3 from W1B\n",
      "scraping page 4 from W1B\n",
      "scraping page 5 from W1B\n",
      "scraping page 6 from W1B\n",
      "------------------------------- SCRAPING COMPLETED FOR W1B\n",
      "Total numbers of properties available in W1B is 120\n",
      "scraping page 1 from W1C\n",
      "scraping page 2 from W1C\n",
      "scraping page 3 from W1C\n",
      "scraping page 4 from W1C\n",
      "------------------------------- SCRAPING COMPLETED FOR W1C\n",
      "Total numbers of properties available in W1C is 80\n",
      "scraping page 1 from W1D\n",
      "scraping page 2 from W1D\n",
      "scraping page 3 from W1D\n",
      "------------------------------- SCRAPING COMPLETED FOR W1D\n",
      "Total numbers of properties available in W1D is 57\n",
      "scraping page 1 from W1F\n",
      "scraping page 2 from W1F\n",
      "------------------------------- SCRAPING COMPLETED FOR W1F\n",
      "Total numbers of properties available in W1F is 40\n",
      "scraping page 1 from W1G\n",
      "scraping page 2 from W1G\n",
      "scraping page 3 from W1G\n",
      "scraping page 4 from W1G\n",
      "scraping page 5 from W1G\n",
      "scraping page 6 from W1G\n",
      "------------------------------- SCRAPING COMPLETED FOR W1G\n",
      "Total numbers of properties available in W1G is 120\n",
      "scraping page 1 from W1H\n",
      "scraping page 2 from W1H\n",
      "scraping page 3 from W1H\n",
      "scraping page 4 from W1H\n",
      "------------------------------- SCRAPING COMPLETED FOR W1H\n",
      "Total numbers of properties available in W1H is 88\n",
      "scraping page 1 from W1J\n",
      "scraping page 2 from W1J\n",
      "scraping page 3 from W1J\n",
      "scraping page 4 from W1J\n",
      "scraping page 5 from W1J\n",
      "scraping page 6 from W1J\n",
      "scraping page 7 from W1J\n",
      "scraping page 8 from W1J\n",
      "scraping page 9 from W1J\n",
      "scraping page 10 from W1J\n",
      "scraping page 11 from W1J\n",
      "scraping page 12 from W1J\n",
      "scraping page 13 from W1J\n",
      "scraping page 14 from W1J\n",
      "------------------------------- SCRAPING COMPLETED FOR W1J\n",
      "Total numbers of properties available in W1J is 302\n",
      "scraping page 1 from W1K\n",
      "scraping page 2 from W1K\n",
      "scraping page 3 from W1K\n",
      "scraping page 4 from W1K\n",
      "scraping page 5 from W1K\n",
      "scraping page 6 from W1K\n",
      "scraping page 7 from W1K\n",
      "scraping page 8 from W1K\n",
      "scraping page 9 from W1K\n",
      "scraping page 10 from W1K\n",
      "scraping page 11 from W1K\n",
      "scraping page 12 from W1K\n",
      "scraping page 13 from W1K\n",
      "------------------------------- SCRAPING COMPLETED FOR W1K\n",
      "Total numbers of properties available in W1K is 294\n",
      "scraping page 1 from W1S\n",
      "scraping page 2 from W1S\n",
      "------------------------------- SCRAPING COMPLETED FOR W1S\n",
      "Total numbers of properties available in W1S is 35\n",
      "scraping page 1 from W1T\n",
      "scraping page 2 from W1T\n",
      "scraping page 3 from W1T\n",
      "scraping page 4 from W1T\n",
      "scraping page 5 from W1T\n",
      "scraping page 6 from W1T\n",
      "scraping page 7 from W1T\n",
      "------------------------------- SCRAPING COMPLETED FOR W1T\n",
      "Total numbers of properties available in W1T is 159\n",
      "scraping page 1 from W1U\n",
      "scraping page 2 from W1U\n",
      "scraping page 3 from W1U\n",
      "scraping page 4 from W1U\n",
      "scraping page 5 from W1U\n",
      "scraping page 6 from W1U\n",
      "scraping page 7 from W1U\n",
      "scraping page 8 from W1U\n",
      "scraping page 9 from W1U\n",
      "scraping page 10 from W1U\n",
      "scraping page 11 from W1U\n",
      "------------------------------- SCRAPING COMPLETED FOR W1U\n",
      "Total numbers of properties available in W1U is 254\n",
      "scraping page 1 from W1W\n",
      "scraping page 2 from W1W\n",
      "scraping page 3 from W1W\n",
      "scraping page 4 from W1W\n",
      "------------------------------- SCRAPING COMPLETED FOR W1W\n",
      "Total numbers of properties available in W1W is 74\n",
      "scraping page 1 from W2\n",
      "scraping page 2 from W2\n",
      "scraping page 3 from W2\n",
      "scraping page 4 from W2\n",
      "scraping page 5 from W2\n",
      "scraping page 6 from W2\n",
      "scraping page 7 from W2\n",
      "scraping page 8 from W2\n",
      "scraping page 9 from W2\n",
      "scraping page 10 from W2\n",
      "scraping page 11 from W2\n",
      "scraping page 12 from W2\n",
      "scraping page 13 from W2\n",
      "scraping page 14 from W2\n",
      "scraping page 15 from W2\n",
      "scraping page 16 from W2\n",
      "scraping page 17 from W2\n",
      "scraping page 18 from W2\n",
      "scraping page 19 from W2\n",
      "scraping page 20 from W2\n",
      "scraping page 21 from W2\n",
      "scraping page 22 from W2\n",
      "scraping page 23 from W2\n",
      "scraping page 24 from W2\n",
      "scraping page 25 from W2\n",
      "scraping page 26 from W2\n",
      "scraping page 27 from W2\n",
      "scraping page 28 from W2\n",
      "scraping page 29 from W2\n",
      "scraping page 30 from W2\n",
      "scraping page 31 from W2\n",
      "scraping page 32 from W2\n",
      "scraping page 33 from W2\n",
      "------------------------------- SCRAPING COMPLETED FOR W2\n",
      "Total numbers of properties available in W2 is 760\n",
      "scraping page 1 from W3\n",
      "scraping page 2 from W3\n",
      "scraping page 3 from W3\n",
      "scraping page 4 from W3\n",
      "scraping page 5 from W3\n",
      "scraping page 6 from W3\n",
      "scraping page 7 from W3\n",
      "------------------------------- SCRAPING COMPLETED FOR W3\n",
      "Total numbers of properties available in W3 is 147\n",
      "scraping page 1 from W4\n",
      "scraping page 2 from W4\n",
      "scraping page 3 from W4\n",
      "scraping page 4 from W4\n",
      "------------------------------- SCRAPING COMPLETED FOR W4\n",
      "Total numbers of properties available in W4 is 91\n",
      "scraping page 1 from W5\n",
      "scraping page 2 from W5\n",
      "scraping page 3 from W5\n",
      "scraping page 4 from W5\n",
      "scraping page 5 from W5\n",
      "scraping page 6 from W5\n",
      "scraping page 7 from W5\n",
      "------------------------------- SCRAPING COMPLETED FOR W5\n",
      "Total numbers of properties available in W5 is 156\n",
      "scraping page 1 from W6\n",
      "scraping page 2 from W6\n",
      "scraping page 3 from W6\n",
      "scraping page 4 from W6\n",
      "scraping page 5 from W6\n",
      "scraping page 6 from W6\n",
      "scraping page 7 from W6\n",
      "scraping page 8 from W6\n",
      "scraping page 9 from W6\n",
      "scraping page 10 from W6\n",
      "scraping page 11 from W6\n",
      "scraping page 12 from W6\n",
      "scraping page 13 from W6\n",
      "scraping page 14 from W6\n",
      "scraping page 15 from W6\n",
      "scraping page 16 from W6\n",
      "scraping page 17 from W6\n",
      "scraping page 18 from W6\n",
      "scraping page 19 from W6\n",
      "scraping page 20 from W6\n",
      "scraping page 21 from W6\n",
      "scraping page 22 from W6\n",
      "scraping page 23 from W6\n",
      "scraping page 24 from W6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping page 25 from W6\n",
      "scraping page 26 from W6\n",
      "scraping page 27 from W6\n",
      "scraping page 28 from W6\n",
      "scraping page 29 from W6\n",
      "scraping page 30 from W6\n",
      "------------------------------- SCRAPING COMPLETED FOR W6\n",
      "Total numbers of properties available in W6 is 684\n",
      "scraping page 1 from W7\n",
      "------------------------------- SCRAPING COMPLETED FOR W7\n",
      "Total numbers of properties available in W7 is 18\n",
      "scraping page 1 from W8\n",
      "scraping page 2 from W8\n",
      "scraping page 3 from W8\n",
      "scraping page 4 from W8\n",
      "scraping page 5 from W8\n",
      "scraping page 6 from W8\n",
      "scraping page 7 from W8\n",
      "scraping page 8 from W8\n",
      "scraping page 9 from W8\n",
      "scraping page 10 from W8\n",
      "scraping page 11 from W8\n",
      "scraping page 12 from W8\n",
      "scraping page 13 from W8\n",
      "scraping page 14 from W8\n",
      "scraping page 15 from W8\n",
      "scraping page 16 from W8\n",
      "scraping page 17 from W8\n",
      "scraping page 18 from W8\n",
      "scraping page 19 from W8\n",
      "scraping page 20 from W8\n",
      "scraping page 21 from W8\n",
      "scraping page 22 from W8\n",
      "scraping page 23 from W8\n",
      "------------------------------- SCRAPING COMPLETED FOR W8\n",
      "Total numbers of properties available in W8 is 520\n",
      "scraping page 1 from W9\n",
      "scraping page 2 from W9\n",
      "scraping page 3 from W9\n",
      "scraping page 4 from W9\n",
      "scraping page 5 from W9\n",
      "scraping page 6 from W9\n",
      "scraping page 7 from W9\n",
      "scraping page 8 from W9\n",
      "------------------------------- SCRAPING COMPLETED FOR W9\n",
      "Total numbers of properties available in W9 is 173\n",
      "scraping page 1 from WC1A\n",
      "------------------------------- SCRAPING COMPLETED FOR WC1A\n",
      "Total numbers of properties available in WC1A is 15\n",
      "scraping page 1 from WC1B\n",
      "------------------------------- SCRAPING COMPLETED FOR WC1B\n",
      "Total numbers of properties available in WC1B is 22\n",
      "scraping page 1 from WC1E\n",
      "------------------------------- SCRAPING COMPLETED FOR WC1E\n",
      "Total numbers of properties available in WC1E is 24\n",
      "scraping page 1 from WC1H\n",
      "scraping page 2 from WC1H\n",
      "scraping page 3 from WC1H\n",
      "scraping page 4 from WC1H\n",
      "------------------------------- SCRAPING COMPLETED FOR WC1H\n",
      "Total numbers of properties available in WC1H is 82\n",
      "scraping page 1 from WC1N\n",
      "------------------------------- SCRAPING COMPLETED FOR WC1N\n",
      "Total numbers of properties available in WC1N is 21\n",
      "scraping page 1 from WC1V\n",
      "------------------------------- SCRAPING COMPLETED FOR WC1V\n",
      "Total numbers of properties available in WC1V is 19\n",
      "scraping page 1 from WC1X\n",
      "scraping page 2 from WC1X\n",
      "scraping page 3 from WC1X\n",
      "------------------------------- SCRAPING COMPLETED FOR WC1X\n",
      "Total numbers of properties available in WC1X is 59\n",
      "scraping page 1 from WC2A\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2A\n",
      "Total numbers of properties available in WC2A is 21\n",
      "scraping page 1 from WC2B\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2B\n",
      "Total numbers of properties available in WC2B is 23\n",
      "scraping page 1 from WC2E\n",
      "scraping page 2 from WC2E\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2E\n",
      "Total numbers of properties available in WC2E is 47\n",
      "scraping page 1 from WC2H\n",
      "scraping page 2 from WC2H\n",
      "scraping page 3 from WC2H\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2H\n",
      "Total numbers of properties available in WC2H is 63\n",
      "scraping page 1 from WC2N\n",
      "scraping page 2 from WC2N\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2N\n",
      "Total numbers of properties available in WC2N is 44\n",
      "scraping page 1 from WC2R\n",
      "scraping page 2 from WC2R\n",
      "------------------------------- SCRAPING COMPLETED FOR WC2R\n",
      "Total numbers of properties available in WC2R is 27\n",
      "------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS 9511\n"
     ]
    }
   ],
   "source": [
    "# create an empty DataFrame outside the function\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# loop through postcodes\n",
    "for postcode in codes.loc[188:]['Postcode district']:\n",
    "    # call the function and pass the empty DataFrame as an argument\n",
    "    df1 = OnTheMarket_rent(postcode, 'Sales', 'OnTheMarket', df)\n",
    "    # append the df1 DataFrame to the empty DataFrame\n",
    "    df = pd.concat([df, df1], ignore_index=True)\n",
    "print('------------------------ WEB SCRAPING COMPLETED: OVERALL TOTAL NUMBER OF PROPERTIES SCRAPED IS ' + str(df.shape[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b9af3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('onTheMarket_rent_data.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2400f58f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Postcode district</th>\n",
       "      <th>Local Areas</th>\n",
       "      <th>Borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>SW1V</td>\n",
       "      <td>Vauxhall Bridge, Chelsea Bridge, Victoria Stat...</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>SW1W</td>\n",
       "      <td>Belgravia</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>SW1X</td>\n",
       "      <td>Belgravia, Knightsbridge</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>SW1Y</td>\n",
       "      <td>St. James's</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>SW2</td>\n",
       "      <td>Brixton, Brixton Hill, Streatham Hill, Tulse H...</td>\n",
       "      <td>Lambeth</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>WC2B</td>\n",
       "      <td>Drury Lane, Aldwych</td>\n",
       "      <td>Camden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>WC2E</td>\n",
       "      <td>Covent Garden</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>WC2H</td>\n",
       "      <td>Leicester Square</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>WC2N</td>\n",
       "      <td>Charing Cross</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>WC2R</td>\n",
       "      <td>Somerset House</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>80 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Postcode district                                        Local Areas   \n",
       "188              SW1V  Vauxhall Bridge, Chelsea Bridge, Victoria Stat...  \\\n",
       "189              SW1W                                          Belgravia   \n",
       "190              SW1X                           Belgravia, Knightsbridge   \n",
       "191              SW1Y                                        St. James's   \n",
       "192               SW2  Brixton, Brixton Hill, Streatham Hill, Tulse H...   \n",
       "..                ...                                                ...   \n",
       "263              WC2B                                Drury Lane, Aldwych   \n",
       "264              WC2E                                      Covent Garden   \n",
       "265              WC2H                                   Leicester Square   \n",
       "266              WC2N                                      Charing Cross   \n",
       "267              WC2R                                     Somerset House   \n",
       "\n",
       "         Borough  \n",
       "188  Westminster  \n",
       "189  Westminster  \n",
       "190  Westminster  \n",
       "191  Westminster  \n",
       "192      Lambeth  \n",
       "..           ...  \n",
       "263       Camden  \n",
       "264  Westminster  \n",
       "265  Westminster  \n",
       "266  Westminster  \n",
       "267  Westminster  \n",
       "\n",
       "[80 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = codes.loc[188:]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3352a30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
